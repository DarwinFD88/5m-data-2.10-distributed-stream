{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlbfpGF5aUh"
      },
      "source": [
        "# Hands-on with Spark (Structured Streaming)\n",
        "\n",
        "![spark](https://cdn-images-1.medium.com/max/300/1*c8CtvqKJDVUnMoPGujF5fA.png)\n",
        "\n",
        "In the previous lesson, we learnt about Spark SQL, Dataframes and Pandas API. In this lesson, we will continue with the Structured Streaming.\n",
        "\n",
        "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.\n",
        "\n",
        "You can use the Dataset/DataFrame API to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides **fast, scalable, fault-tolerant, end-to-end exactly-once** stream processing without the user having to reason about streaming.\n",
        "\n",
        "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0M79JSa6Oqv"
      },
      "source": [
        "## Installing and Initializing Spark\n",
        "\n",
        "First, like previously, we'll need to install Spark and its dependencies:\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with Hadoop\n",
        "3.   Findspark (used to locate the Spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KE17krsq6Rd6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
        "\n",
        "# set the options to connect to our Kafka cluster\n",
        "options = {\n",
        "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"YnJhdmUtZmlzaC0xMTQ2MyQSvwXBuLOQsV1W7YffuC8cDaZcA3fKQwakMhnQGgg\" password=\"MDUxNjc4YzEtYzYxNy00NTE1LWEwNWYtMDBhODRlZmE0OGJm\";',\n",
        "    # \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
        "    # \"kafka.security.protocol\" : \"SASL_SSL\",\n",
        "    \"kafka.bootstrap.servers\": 'localhost:9092',\n",
        "    \"subscribe\": 'pizza-orders',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra8Ncwms6UsD"
      },
      "outputs": [],
      "source": [
        "# # findspark is only required if you are using a standalone Spark installation (downloaded tar.gz)\n",
        "# import findspark\n",
        "# findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below cell may take a few minutes to run. It is slow because:\n",
        "- Package downloading: Your PYSPARK_SUBMIT_ARGS includes a Kafka package that needs to be downloaded from Maven repositories on first run\n",
        "- JVM cold start: Initial JVM startup is always slow\n",
        "- Jupyter overhead: Running in a notebook adds some initialization overhead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lhi1xuPB6VfD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/12/10 21:23:32 WARN Utils: Your hostname, DESKTOP-4VD7137 resolves to a loopback address: 127.0.1.1; using 172.26.8.82 instead (on interface eth0)\n",
            "25/12/10 21:23:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            ":: loading settings :: url = jar:file:/home/darwin/miniconda3/envs/kafka/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /home/darwin/.ivy2/cache\n",
            "The jars for the packages stored in: /home/darwin/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8d44b8f6-9d04-490f-894c-783013d05797;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
            "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (169ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (86ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (662ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (71ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (117ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (2722ms)\n",
            "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
            "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (248ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (1273ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (142ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (1272ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
            "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (132ms)\n",
            ":: resolution report :: resolve 14618ms :: artifacts dl 6936ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-8d44b8f6-9d04-490f-894c-783013d05797\n",
            "\tconfs: [default]\n",
            "\t11 artifacts copied, 0 already retrieved (56767kB/799ms)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/12/10 21:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "# We will only use 2 cores below to speed up creation of the spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"LearnSparkStreaming\").master(\"local[2]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "VYTXdMSv6cpC",
        "outputId": "e0817f69-3087-4e2c-e643-5ba3ed477a11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://172.26.8.82:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>LearnSparkStreaming</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e8636382f80>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36cRSgfTNXXA"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Batch\" Mode\n",
        "\n",
        "Let's start with reading and analyzing our `pizza-orders` kafka topic in the usual \"batch\" mode of Spark SQL and Dataframes. This is akin to the batch queries we did in the previous lesson. In this case we are taking the messages with the earliest to latest offsets of the topic as a single \"batch\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6fZDMWC26dJ0"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.read.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJLqrFeXOXpG",
        "outputId": "56b04137-b3cc-4231-e23a-ea7f16977956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "|                 key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     0|2025-12-10 20:50:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     1|2025-12-10 20:50:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     2|2025-12-10 20:50:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     3|2025-12-10 20:50:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     4|2025-12-10 20:50:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     5|2025-12-10 21:00:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     6|2025-12-10 21:00:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     7|2025-12-10 21:00:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     8|2025-12-10 21:00:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     9|2025-12-10 21:00:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    10|2025-12-10 21:07:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    11|2025-12-10 21:07:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    12|2025-12-10 21:07:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    13|2025-12-10 21:07:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|    14|2025-12-10 21:07:...|            0|\n",
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--uyDySFSEB",
        "outputId": "30d90a0a-8d05-4ae7-8ad6-dcdba0677464"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 1:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                 key|               value|\n",
            "+--------------------+--------------------+\n",
            "|{\"shop\": \"Ill Mak...|{\"id\": 0, \"shop\":...|\n",
            "|{\"shop\": \"Ill Mak...|{\"id\": 1, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 2, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 3, \"shop\":...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 4, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 0, \"shop\":...|\n",
            "|{\"shop\": \"Its-a m...|{\"id\": 1, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 2, \"shop\":...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 3, \"shop\":...|\n",
            "|{\"shop\": \"Ill Mak...|{\"id\": 4, \"shop\":...|\n",
            "|{\"shop\": \"Its-a m...|{\"id\": 0, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 1, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 2, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 3, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 4, \"shop\":...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "pizza_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m_zjPQ6_FUVE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType, StructType, ArrayType, StructField"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "owyJGHliQdBG"
      },
      "outputs": [],
      "source": [
        "pizza_schema = StructType([\n",
        "  StructField(\"pizzaName\", StringType()),\n",
        "  StructField(\"additionalToppings\", ArrayType(StringType())),\n",
        "])\n",
        "\n",
        "order_schema = StructType([\n",
        "  StructField(\"address\", StringType()),\n",
        "  StructField(\"id\", IntegerType()),\n",
        "  StructField(\"name\", StringType()),\n",
        "  StructField(\"phoneNumber\", StringType()),\n",
        "  StructField(\"shop\", StringType()),\n",
        "  StructField(\"cost\", DoubleType()),\n",
        "  StructField(\"pizzas\", ArrayType(pizza_schema)),\n",
        "  StructField(\"timestamp\", LongType()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8_KAqsS7Q4_5"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7MWOtWPRM-0",
        "outputId": "af678ae7-e0be-4117-88b9-27cdd9fd8e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- id: integer (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- phoneNumber: string (nullable = true)\n",
            " |    |-- shop: string (nullable = true)\n",
            " |    |-- cost: double (nullable = true)\n",
            " |    |-- pizzas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- pizzaName: string (nullable = true)\n",
            " |    |    |    |-- additionalToppings: array (nullable = true)\n",
            " |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mio4VUyERh1H",
        "outputId": "36c7f7d8-3f03-41dd-9425-5859d45bd940"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|timestamp              |value                                                                                                                                                                                                                                                                                                                          |\n",
            "+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|2025-12-10 20:50:03.411|{21087 Calvin Plains\\nJonesland, NY 76392, 0, Jessica Smith, 001-701-915-3000, Ill Make You a Pizza You Cant Refuse, 1.82, [{Salami, []}], 1765371003381}                                                                                                                                                                      |\n",
            "|2025-12-10 20:50:05.414|{19721 Drew Key\\nNew Donaldport, NH 05690, 1, Roger Brown, 475-943-3780x8105, Ill Make You a Pizza You Cant Refuse, 2.71, [{Mari & Monti, [ðŸ“ strawberry, ðŸŸ tuna]}], 1765371005414}                                                                                                                                           |\n",
            "|2025-12-10 20:50:07.418|{278 Phillips Crossing Apt. 661\\nPort Maryfurt, GU 86535, 2, Matthew Williams, 924-927-0965x759, Luigis Pizza, 34.83, [{Margherita, [ðŸ§€ blue cheese, ðŸŒ banana]}], 1765371007418}                                                                                                                                              |\n",
            "|2025-12-10 20:50:08.421|{547 Brenda Walks\\nLake Ronaldborough, CT 52490, 3, Corey Rodriguez, 001-629-242-5711x630, Luigis Pizza, 9.25, [{Mari & Monti, [ðŸŒ banana, ðŸŸ tuna, ðŸ«‘ green peppers]}], 1765371008420}                                                                                                                                        |\n",
            "|2025-12-10 20:50:10.424|{752 Brandon Expressway Apt. 815\\nNorth Elizabeth, AR 84161, 4, Stephen Stevens, 253.786.2934, Circular Pi Pizzeria, 5.84, [{Peperoni, [ðŸ«‘ green peppers]}], 1765371010424}                                                                                                                                                    |\n",
            "|2025-12-10 21:00:00.584|{21087 Calvin Plains\\nJonesland, NY 76392, 0, Jessica Smith, 001-701-915-3000, Mammamia Pizza, 21.33, [{Margherita, [ðŸ§€ blue cheese, ðŸ“ strawberry, ðŸŸ tuna]}, {Peperoni, [ðŸ§… onion, ðŸ¥“ bacon]}], 1765371600546}                                                                                                               |\n",
            "|2025-12-10 21:00:02.589|{19721 Drew Key\\nNew Donaldport, NH 05690, 1, Roger Brown, 475-943-3780x8105, Its-a me! Mario Pizza!, 11.46, [{Marinara, [ðŸ¥š egg]}], 1765371602588}                                                                                                                                                                            |\n",
            "|2025-12-10 21:00:04.593|{278 Phillips Crossing Apt. 661\\nPort Maryfurt, GU 86535, 2, Matthew Williams, 924-927-0965x759, Mammamia Pizza, 43.16, [{Mari & Monti, []}, {Peperoni, []}, {Marinara, []}], 1765371604593}                                                                                                                                   |\n",
            "|2025-12-10 21:00:06.598|{547 Brenda Walks\\nLake Ronaldborough, CT 52490, 3, Corey Rodriguez, 001-629-242-5711x630, Circular Pi Pizzeria, 40.62, [{Salami, [ðŸ“ strawberry, ðŸ§€ blue cheese]}, {Diavola, [ðŸ¥š egg, ðŸ¥“ bacon, ðŸ¥š egg]}, {Peperoni, [ðŸ§„ garlic, ðŸ¥š egg]}, {Peperoni, [ðŸ§€ blue cheese]}, {Salami, [ðŸ¥š egg, ðŸ«‘ green peppers]}], 1765371606597}|\n",
            "|2025-12-10 21:00:08.6  |{752 Brandon Expressway Apt. 815\\nNorth Elizabeth, AR 84161, 4, Stephen Stevens, 253.786.2934, Ill Make You a Pizza You Cant Refuse, 22.93, [{Margherita, []}, {Diavola, [ðŸ§„ garlic]}, {Marinara, []}, {Mari & Monti, [ðŸŒ¶ï¸ hot pepper, ðŸ… tomato, ðŸ“ strawberry]}, {Margherita, [ðŸ§„ garlic, ðŸ… tomato]}], 1765371608600}       |\n",
            "|2025-12-10 21:07:27.359|{144 Ross Port\\nAliciafurt, KY 28535, 0, Ricky Sexton, 8667536484, Its-a me! Mario Pizza!, 5.66, [{Salami, [ðŸ«’ olives, ðŸ§€ blue cheese, ðŸŒ¶ï¸ hot pepper]}, {Mari & Monti, [ðŸ§„ garlic]}, {Salami, [ðŸ«’ olives, ðŸ“ strawberry]}, {Mari & Monti, []}, {Margherita, [ðŸ«‘ green peppers]}], 1765372047348}                              |\n",
            "|2025-12-10 21:07:28.364|{49595 Gary Port Suite 707\\nNorth Michaelfort, CA 18853, 1, Russell Sanders, (873)972-6401x764, Luigis Pizza, 17.53, [{Marinara, [ðŸ pineapple]}, {Marinara, [ðŸ§… onion, ðŸ¥“ bacon]}, {Salami, [ðŸ… tomato, ðŸ«’ olives]}], 1765372048363}                                                                                          |\n",
            "|2025-12-10 21:07:29.376|{4189 Madison Glens Suite 206\\nLake Vickiborough, TN 20792, 2, Stephanie Rivera, (544)795-9493x841, Luigis Pizza, 24.21, [{Marinara, [ðŸŒ¶ï¸ hot pepper, ðŸ«’ olives, ðŸ… tomato]}, {Peperoni, [ðŸŒ banana, ðŸ§€ blue cheese, ðŸ… tomato]}], 1765372049376}                                                                              |\n",
            "|2025-12-10 21:07:31.38 |{77289 Dustin Villages\\nLake Frank, NY 99386, 3, Kelli Hamilton DDS, 244-779-3581, Mammamia Pizza, 12.74, [{Diavola, [ðŸ pineapple, ðŸ¥š egg]}, {Peperoni, [ðŸŒ¶ï¸ hot pepper]}, {Diavola, []}, {Diavola, [ðŸ“ strawberry, ðŸ§… onion]}], 1765372051380}                                                                               |\n",
            "|2025-12-10 21:07:33.385|{8697 Anthony Valley\\nPort Kellymouth, FL 64221, 4, Patricia Castaneda, 328-798-9970x51560, Mammamia Pizza, 1.67, [{Margherita, [ðŸ§€ blue cheese]}, {Margherita, [ðŸ¥“ bacon, ðŸŒ¶ï¸ hot pepper, ðŸ«’ olives]}, {Mari & Monti, [ðŸ«‘ green peppers]}], 1765372053385}                                                                    |\n",
            "+-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdYUKmrygE7_"
      },
      "source": [
        "We can use _dot notation_ to select the field within a `Struct`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt_TUX3_edrK",
        "outputId": "8907d09f-371c-4c04-8912-a91348b40c3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "| cost|\n",
            "+-----+\n",
            "| 1.82|\n",
            "| 2.71|\n",
            "|34.83|\n",
            "| 9.25|\n",
            "| 5.84|\n",
            "|21.33|\n",
            "|11.46|\n",
            "|43.16|\n",
            "|40.62|\n",
            "|22.93|\n",
            "| 5.66|\n",
            "|17.53|\n",
            "|24.21|\n",
            "|12.74|\n",
            "| 1.67|\n",
            "+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.select(\"value.cost\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgfNZXpVgXz_"
      },
      "source": [
        "Computing the \"total revenue\" per shop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqZHVVFggUYv",
        "outputId": "5c140144-c405-44d5-ff32-e38bdd0cab0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 4:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------+-----------------------+\n",
            "|shop                                |sum(value.cost AS cost)|\n",
            "+------------------------------------+-----------------------+\n",
            "|Ill Make You a Pizza You Cant Refuse|27.46                  |\n",
            "|Luigis Pizza                        |85.82                  |\n",
            "|Mammamia Pizza                      |78.89999999999999      |\n",
            "|Its-a me! Mario Pizza!              |17.12                  |\n",
            "|Circular Pi Pizzeria                |46.459999999999994     |\n",
            "+------------------------------------+-----------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.groupBy(\"value.shop\").sum(\"value.cost\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuvYqRChU_v"
      },
      "source": [
        "> 1. Count the no. of orders by shop.\n",
        "> 2. Compute the avg revenue by shop and sort by highest to lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fEuCpW_Pgk1Q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8IhcetFiY24",
        "outputId": "7af16123-be01-4746-e028-2d0f66212964"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+-----------------------+\n",
            "|min(timestamp)         |max(timestamp)         |\n",
            "+-----------------------+-----------------------+\n",
            "|2025-12-10 20:50:03.411|2025-12-10 21:07:33.385|\n",
            "+-----------------------+-----------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.select(min(\"timestamp\"), max(\"timestamp\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MDIcbPs74B3"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Streaming\" Mode\n",
        "\n",
        "The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the *unbounded input table*. Letâ€™s understand this model in more detail.\n",
        "\n",
        "Consider the input data stream as the â€œInput Tableâ€. Every data item that is arriving on the stream is like a new row being appended to the Input Table.\n",
        "\n",
        "![concept](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
        "\n",
        "A query on the input will generate the â€œResult Tableâ€. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n",
        "\n",
        "![result table](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
        "\n",
        "To illustrate the use of this model, letâ€™s understand the model in context of a word count model. The first lines DataFrame is the input table, and the final wordCounts DataFrame is the result table. Note that the query on streaming lines DataFrame to generate wordCounts is exactly the same as it would be a static DataFrame. However, when this query is started, Spark will continuously check for new data from the socket connection. If there is new data, Spark will run an â€œincrementalâ€ query that combines the previous running counts with the new data to compute updated counts, as shown below.\n",
        "\n",
        "![example](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9A-q7BY9F_F"
      },
      "source": [
        "Event-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time.\n",
        "\n",
        "For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them.\n",
        "\n",
        "This event-time is very naturally expressed in this model â€“ each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column â€“ each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\n",
        "\n",
        "Furthermore, this model naturally handles data that has arrived later than expected based on its event-time. Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data, as well as cleaning up old aggregates to limit the size of intermediate state data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TKfF1Mc9STFX"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss8wgrNDkwuW",
        "outputId": "d0f3d31d-d229-46f7-db9b-9a893ca8f44b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pizza_df.isStreaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grlYCt8Ujg3G",
        "outputId": "3048feba-75ec-40bd-9f1f-6b9bd8788622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G_vL4jkTjl2v"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ETDCxskFZ9",
        "outputId": "9b7b3a27-13c7-447a-d5cd-2a02d0d55892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- id: integer (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- phoneNumber: string (nullable = true)\n",
            " |    |-- shop: string (nullable = true)\n",
            " |    |-- cost: double (nullable = true)\n",
            " |    |-- pizzas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- pizzaName: string (nullable = true)\n",
            " |    |    |    |-- additionalToppings: array (nullable = true)\n",
            " |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zY8eDSs9kLan",
        "outputId": "c4ae7110-9517-4f0d-bf8a-d767fb4c8c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/12/10 21:37:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f40e8a2a-2e03-4306-b692-b45e6590538a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/12/10 21:37:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---------+-----+\n",
            "|timestamp|value|\n",
            "+---------+-----+\n",
            "+---------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-12-10 21:38:...|{409 Ward Stream\\...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-12-10 21:38:...|{272 Tammy Plains...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 13:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-12-10 21:38:...|{3156 Dawson Knol...|\n",
            "|2025-12-10 21:38:...|{0259 Brandon Val...|\n",
            "|2025-12-10 21:38:...|{797 Shaw Junctio...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "query = parsed_df \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# query.awaitTermination() # stops the script from exiting. But this is not required in Jupyter because the kernel stays alive. In real Spark applications, you need awaitTermination() to keep the job running.\n",
        "# query.isActive\n",
        "# query.recentProgress\n",
        "# query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:37:41.526Z',\n",
              "  'batchId': 0,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'addBatch': 2384,\n",
              "   'getBatch': 58,\n",
              "   'latestOffset': 3635,\n",
              "   'queryPlanning': 455,\n",
              "   'triggerExecution': 7343,\n",
              "   'walCommit': 178},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': None,\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:37:59.021Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:09.023Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:19.126Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:29.132Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:39.139Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:48.634Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 76.92307692307692,\n",
              "  'processedRowsPerSecond': 0.3895597974289053,\n",
              "  'durationMs': {'addBatch': 2101,\n",
              "   'getBatch': 1,\n",
              "   'latestOffset': 2,\n",
              "   'queryPlanning': 35,\n",
              "   'triggerExecution': 2567,\n",
              "   'walCommit': 106},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 16}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 16}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 76.92307692307692,\n",
              "    'processedRowsPerSecond': 0.3895597974289053,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:51.206Z',\n",
              "  'batchId': 2,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.38880248833592534,\n",
              "  'processedRowsPerSecond': 0.36616623947272064,\n",
              "  'durationMs': {'addBatch': 1785,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 16,\n",
              "   'queryPlanning': 92,\n",
              "   'triggerExecution': 2731,\n",
              "   'walCommit': 364},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 16}},\n",
              "    'endOffset': {'pizza-orders': {'0': 17}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 17}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.38880248833592534,\n",
              "    'processedRowsPerSecond': 0.36616623947272064,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:38:53.952Z',\n",
              "  'batchId': 3,\n",
              "  'numInputRows': 3,\n",
              "  'inputRowsPerSecond': 1.0924981791697015,\n",
              "  'processedRowsPerSecond': 1.0810810810810811,\n",
              "  'durationMs': {'addBatch': 1409,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 33,\n",
              "   'queryPlanning': 69,\n",
              "   'triggerExecution': 2775,\n",
              "   'walCommit': 278},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 17}},\n",
              "    'endOffset': {'pizza-orders': {'0': 20}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 20}},\n",
              "    'numInputRows': 3,\n",
              "    'inputRowsPerSecond': 1.0924981791697015,\n",
              "    'processedRowsPerSecond': 1.0810810810810811,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 3}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:39:06.735Z',\n",
              "  'batchId': 4,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 20}},\n",
              "    'endOffset': {'pizza-orders': {'0': 20}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 20}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:39:16.747Z',\n",
              "  'batchId': 4,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 20}},\n",
              "    'endOffset': {'pizza-orders': {'0': 20}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 20}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:39:26.758Z',\n",
              "  'batchId': 4,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 6, 'triggerExecution': 7},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 20}},\n",
              "    'endOffset': {'pizza-orders': {'0': 20}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 20}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': 'f88b3cd6-64e9-4989-8290-ce9fe0afcedc',\n",
              "  'runId': '50dc4710-0cb7-47c6-b8ac-2ec7ee54a679',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-12-10T13:39:36.770Z',\n",
              "  'batchId': 4,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 20}},\n",
              "    'endOffset': {'pizza-orders': {'0': 20}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 20}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@743aeabe',\n",
              "   'numOutputRows': 0}}]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query.recentProgress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In the next few cells, we will provide a complete example of counting the number of orders per pizza shop, as the orders stream in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First check if any streams are active\n",
        "spark.streams.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If there are any active streams, stop them\n",
        "for q in spark.streams.active:\n",
        "    print(f\"Stopping query: {q.name}\")\n",
        "    q.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/12/10 21:41:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-be26483e-1ef9-4d49-9343-bc7c8e4b57b7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/12/10 21:41:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+----+-----+\n",
            "|shop|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+--------------+-----+\n",
            "|          shop|count|\n",
            "+--------------+-----+\n",
            "|Mammamia Pizza|    1|\n",
            "+--------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|        Luigis Pizza|    1|\n",
            "|      Mammamia Pizza|    1|\n",
            "|Its-a me! Mario P...|    1|\n",
            "|        Marios Pizza|    2|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|        Luigis Pizza|    1|\n",
            "|      Mammamia Pizza|    1|\n",
            "|Its-a me! Mario P...|    1|\n",
            "|        Marios Pizza|    3|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 4\n",
            "-------------------------------------------\n",
            "+--------------------+-----+\n",
            "|                shop|count|\n",
            "+--------------------+-----+\n",
            "|        Luigis Pizza|    1|\n",
            "|      Mammamia Pizza|    3|\n",
            "|Its-a me! Mario P...|    3|\n",
            "|        Marios Pizza|    3|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/darwin/miniconda3/envs/kafka/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/home/darwin/miniconda3/envs/kafka/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/home/darwin/miniconda3/envs/kafka/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# outputMode(\"complete\") rewrites all aggregated results every trigger â€” good for full snapshots.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# outputMode(\"update\") - only updated rows are written\u001b[39;00m\n\u001b[1;32m     12\u001b[0m query \u001b[38;5;241m=\u001b[39m shop_counts \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 18\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.10/site-packages/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/kafka/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Complete example with counting\n",
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()\n",
        "\n",
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\")) #.groupBy(\"value.shop\").count()\n",
        "\n",
        "shop_counts = parsed_df.groupBy(\"value.shop\").count()\n",
        "\n",
        "# outputMode(\"complete\") rewrites all aggregated results every trigger â€” good for full snapshots.\n",
        "# outputMode(\"update\") - only updated rows are written\n",
        "query = shop_counts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have come to the end of this exercise.\n",
        "\n",
        "To delete the Kafka topic `pizza-orders`, use the command below in your terminal:\n",
        "\n",
        "`./kafka-topics.sh --delete --topic pizza-orders --bootstrap-server localhost:9092`\n",
        "\n",
        "To exit Kafka in your terminal, type `exit`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kafka",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
